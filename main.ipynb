{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import imageio\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import datasets, layers, models, optimizers, regularizers, callbacks\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from sklearn.metrics import confusion_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we create a dict which consists label for each image path and its particular data.\n",
    "def load_file(file_path, label):\n",
    "\n",
    "    # declare the folder name\n",
    "    folder_name = file_path.split(\"/\")[-1]\n",
    "    # declare output list\n",
    "    out_list = []\n",
    "    # load every file that .png format\n",
    "    for image_path in glob.glob(file_path + \"/*.*\"):\n",
    "        # read image file\n",
    "        image = imageio.imread(image_path)\n",
    "        # declare temporary dict dtype\n",
    "        temp = {}\n",
    "        # set the file name\n",
    "        temp[\"name\"] = image_path.split(\"/\")[-1]\n",
    "        # set the file label, 0 for non defect. 1 for defect\n",
    "        temp[\"label\"] = label\n",
    "\n",
    "        # There are somes images are tensor dtype\n",
    "        # Thus I fix by selecting only a tensor index zero\n",
    "        try:   \n",
    "            temp[\"data\"] = image[:,:,0].astype(\"int\") \n",
    "        except:\n",
    "            # normal case\n",
    "            temp[\"data\"] = image.astype(\"int\")\n",
    "        # append temp into output list\n",
    "        out_list.append(temp)\n",
    "    # print process status by checking size of output list\n",
    "    if len(out_list) == 0:\n",
    "        print(\"loading files from folder: {} is failed\".format(folder_name))\n",
    "    else:\n",
    "        print(\"loading file from folder: {} is successful\".format(folder_name))\n",
    "    # convert list into numpy array dtype\n",
    "    return np.array(out_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "defect_images_path      =  r\"path_of_defected_images\"\n",
    "non_defect_images_path1 =  r\"path_of_non_defect_resized_images\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading file from folder: A:\\TechieYan projects\\AI\\Fabric_Fault_detection\\Fabric 2\\augmented_images is successful\n",
      "loading file from folder: A:\\TechieYan projects\\AI\\Fabric_Fault_detection\\Fabric 2\\non_defect_resized_images is successful\n"
     ]
    }
   ],
   "source": [
    "defect_images = load_file(file_path=defect_images_path, label=1)\n",
    "non_defect_images = load_file(file_path=non_defect_images_path1, label=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2251,)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "defect_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000,)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "non_defect_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([{'name': 'A:\\\\TechieYan projects\\\\AI\\\\Fabric_Fault_detection\\\\Fabric 2\\\\augmented_images\\\\aug_0_10.png', 'label': 1, 'data': Array([[ 63,  49,  49, ...,  75,  79,  75],\n",
       "              [ 83,  72,  56, ...,  78,  77,  71],\n",
       "              [ 96,  88,  79, ...,  79,  73,  67],\n",
       "              ...,\n",
       "              [100,  97,  95, ...,  33,  34,  39],\n",
       "              [ 98,  95,  94, ...,  39,  29,  28],\n",
       "              [ 96,  94,  94, ...,  69,  56,  39]])}                                                                                                                           ,\n",
       "       {'name': 'A:\\\\TechieYan projects\\\\AI\\\\Fabric_Fault_detection\\\\Fabric 2\\\\augmented_images\\\\aug_0_1002.png', 'label': 1, 'data': Array([[96, 95, 78, ..., 29, 28, 27],\n",
       "              [94, 96, 86, ..., 20, 16, 16],\n",
       "              [88, 96, 95, ..., 20, 21, 24],\n",
       "              ...,\n",
       "              [72, 71, 72, ..., 34, 29, 21],\n",
       "              [77, 80, 79, ..., 35, 32, 25],\n",
       "              [78, 77, 75, ..., 37, 34, 29]])}                                                                                                                             ,\n",
       "       {'name': 'A:\\\\TechieYan projects\\\\AI\\\\Fabric_Fault_detection\\\\Fabric 2\\\\augmented_images\\\\aug_0_1006.png', 'label': 1, 'data': Array([[186, 186, 186, ...,  52,  43,  45],\n",
       "              [186, 186, 186, ...,  49,  42,  49],\n",
       "              [184, 184, 185, ...,  45,  41,  54],\n",
       "              ...,\n",
       "              [171, 171, 171, ...,  39,  39,  34],\n",
       "              [171, 171, 171, ...,  38,  34,  26],\n",
       "              [171, 171, 171, ...,  40,  33,  27]])}                                                                                                                             ,\n",
       "       ...,\n",
       "       {'name': 'A:\\\\TechieYan projects\\\\AI\\\\Fabric_Fault_detection\\\\Fabric 2\\\\augmented_images\\\\aug_0_9982.png', 'label': 1, 'data': Array([[189, 203, 217, ..., 202, 194, 174],\n",
       "              [198, 193, 193, ..., 196, 175, 171],\n",
       "              [193, 197, 197, ..., 177, 171, 175],\n",
       "              ...,\n",
       "              [173, 161, 143, ...,  93,  77,  80],\n",
       "              [162, 145, 130, ...,  97,  88, 113],\n",
       "              [146, 132, 126, ..., 110, 114, 111]])}                                                                                                                             ,\n",
       "       {'name': 'A:\\\\TechieYan projects\\\\AI\\\\Fabric_Fault_detection\\\\Fabric 2\\\\augmented_images\\\\aug_0_9987.png', 'label': 1, 'data': Array([[251, 246, 246, ..., 142, 147, 145],\n",
       "              [249, 253, 248, ..., 146, 146, 142],\n",
       "              [250, 246, 250, ..., 147, 143, 140],\n",
       "              ...,\n",
       "              [176, 174, 165, ..., 129, 126, 136],\n",
       "              [175, 168, 155, ..., 133, 142, 153],\n",
       "              [171, 158, 146, ..., 134, 143, 151]])}                                                                                                                             ,\n",
       "       {'name': 'A:\\\\TechieYan projects\\\\AI\\\\Fabric_Fault_detection\\\\Fabric 2\\\\augmented_images\\\\aug_0_9988.png', 'label': 1, 'data': Array([[ 67,  67,  70, ..., 194, 194, 194],\n",
       "              [ 67,  67,  70, ..., 194, 194, 194],\n",
       "              [ 67,  67,  70, ..., 194, 194, 194],\n",
       "              ...,\n",
       "              [106, 107, 108, ..., 246, 246, 245],\n",
       "              [119, 120, 121, ..., 240, 239, 239],\n",
       "              [137, 138, 139, ..., 232, 230, 230]])}                                                                                                                             ],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "defect_images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Prepare and clean the data to avoid error during model fitting.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Size: 2000\n"
     ]
    }
   ],
   "source": [
    "np.random.shuffle(non_defect_images)\n",
    "np.random.shuffle(defect_images)\n",
    "\n",
    "# the class size is the min length compared with defect-free and defect images, we do this in orde to balance the dataset.\n",
    "if defect_images.shape[0] <= non_defect_images.shape[0]:\n",
    "  class_size = defect_images.shape[0]\n",
    "else:\n",
    "  class_size = non_defect_images.shape[0]\n",
    "print(\"Class Size:\", class_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Size: 2000\n",
      "(4000, 255, 255, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([0, 1]), array([2000, 2000], dtype=int64))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we shuffle the order of defect-free and defect images\n",
    "np.random.shuffle(non_defect_images)\n",
    "np.random.shuffle(defect_images)\n",
    "\n",
    "# the class size is the min length compared with defect-free and defect images, we do this in orde to balance the dataset.\n",
    "if defect_images.shape[0] <= non_defect_images.shape[0]:\n",
    "  class_size = defect_images.shape[0] \n",
    "else:\n",
    "  non_defect_images.shape[0]\n",
    "print(\"Class Size:\", class_size)\n",
    "\n",
    "# Concatenate both the datasets with size as class_size.\n",
    "dataset = np.concatenate((defect_images[:class_size], non_defect_images[:class_size]))\n",
    "\n",
    "# create an empty matrix X of 256x4096 and has dataset length row, which holds all the data i.e images from dataset.\n",
    "# Independent Features -> X\n",
    "X = np.empty([dataset.shape[0], 255, 255]).astype(int)\n",
    "\n",
    "# create vector y which has dataset length, which holds all the labels for our data, this is jsut similar to partitioning the data before splitting, \n",
    "# Target_variable -> y\n",
    "y = np.empty(dataset.shape[0]).astype(int)\n",
    "\n",
    "# assign the X,y one-by-one\n",
    "for i in range(dataset.shape[0]):\n",
    "    X[i] = dataset[i][\"data\"]\n",
    "    y[i] = dataset[i][\"label\"]\n",
    "\n",
    "# since Keras acquire the Image input in a tensor type -> we reshape X\n",
    "X = X.reshape(X.shape[0], 255, 255, 1)\n",
    "print(X.shape)\n",
    "\n",
    "# display size of the label 0 and label 1 \n",
    "np.unique(y, return_counts=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Model Building**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 249, 249, 16)      800       \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 249, 249, 16)     64        \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " activation (Activation)     (None, 249, 249, 16)      0         \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 124, 124, 16)     0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 124, 124, 16)      0         \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 124, 124, 32)      12832     \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 124, 124, 32)     128       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_1 (Activation)   (None, 124, 124, 32)      0         \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 62, 62, 32)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 62, 62, 32)        0         \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 62, 62, 64)        51264     \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 62, 62, 64)       256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_2 (Activation)   (None, 62, 62, 64)        0         \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 31, 31, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 31, 31, 64)        0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 61504)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                3936320   \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 64)               256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_3 (Activation)   (None, 64)                0         \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,001,985\n",
      "Trainable params: 4,001,633\n",
      "Non-trainable params: 352\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.engine.sequential.Sequential at 0x2785677f400>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_model(image_shape=(256, 4096, 1), print_summary=True):\n",
    "    # initial model\n",
    "    model = models.Sequential()\n",
    "\n",
    "    # CONV layer: filter 16, stride 7x7\n",
    "    model.add(layers.Conv2D(16, (7, 7),input_shape=image_shape))\n",
    "    # Batch Normalization layer -> avoid overfitting\n",
    "    model.add(layers.BatchNormalization())\n",
    "    # activation layer \n",
    "    model.add(layers.Activation('relu'))\n",
    "    # max pooling -> reduce image size\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    # droput later -> avoid overfitting\n",
    "    model.add(layers.Dropout(0.25))\n",
    "\n",
    "\n",
    "    # CONV layer: filter 32, stride 5x5\n",
    "    model.add(layers.Conv2D(32, (5, 5), padding=\"same\"))\n",
    "    # Batch Normalization layer -> avoid overfitting\n",
    "    model.add(layers.BatchNormalization())\n",
    "    # activation layer \n",
    "    model.add(layers.Activation('relu'))\n",
    "    # max pooling -> reduce image size\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    # droput later -> avoid overfitting\n",
    "    model.add(layers.Dropout(0.25))\n",
    "    \n",
    "    # CONV layer: filter 64, stride 5x5\n",
    "    model.add(layers.Conv2D(64, (5, 5), padding=\"same\"))\n",
    "    # Batch Normalization layer -> avoid overfitting\n",
    "    model.add(layers.BatchNormalization())\n",
    "    # activation layer \n",
    "    model.add(layers.Activation('relu'))\n",
    "    # max pooling -> reduce image size\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    # droput later -> avoid overfitting\n",
    "    model.add(layers.Dropout(0.25))\n",
    "\n",
    "\n",
    "\n",
    "    # flatten layer -> To convert from matrix to vector ie from multidimensional array to 1D array with single column.\n",
    "    model.add(layers.Flatten())\n",
    "    \n",
    "\n",
    "    # Here we are creating an actual neural network which takes input as the flatten layer.\n",
    "    # fully connected layer -> nn layer with 64 nodes\n",
    "    model.add(layers.Dense(64))\n",
    "    # Batch Normalization layer -> avoid overfitting\n",
    "    model.add(layers.BatchNormalization())\n",
    "    # activation layer \n",
    "    model.add(layers.Activation('relu'))\n",
    "    # droput later -> avoid overfitting\n",
    "    model.add(layers.Dropout(0.25))\n",
    "\n",
    "\n",
    "    # output layer\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    \n",
    "    # set model compiler\n",
    "    model.compile(optimizer='Adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    # show the CNN model detail\n",
    "    if print_summary:\n",
    "        model.summary()\n",
    "    return model\n",
    "\n",
    "def train_model(model, xtrain, ytrain, xval, yval, n_epoch, batch_size):\n",
    "    # train CNN model\n",
    "    # batch size to reduce memory usage\n",
    "    # set early stopping to avoid overfitting\n",
    "    \n",
    "    earlystopping = EarlyStopping(monitor='val_accuracy', patience=2)\n",
    "    filepath = \"/content/drive/MyDrive/Fabric_Fault_detection/Fabric 2/model/weights-best-{epoch:02d}-{val_accuracy:.2f}.hdf5\"\n",
    "    \n",
    "    checkpoint = ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "    callbacks_list = [checkpoint, earlystopping]\n",
    "\n",
    "    history = model.fit(xtrain, ytrain, epochs=n_epoch, batch_size=batch_size, validation_data=(xval, yval), callbacks=[callbacks_list])\n",
    "    return history\n",
    "\n",
    "create_model(image_shape=(255, 255, 1), print_summary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Export CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_train: number of samples each class: (array([0, 1]), array([1600, 1600], dtype=int64))\n",
      "y_test: number of samples each class: (array([0, 1]), array([400, 400], dtype=int64))\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=23)\n",
    "print(\"y_train: number of samples each class: {}\".format(np.unique(y_train, return_counts=True)))\n",
    "print(\"y_test: number of samples each class: {}\".format(np.unique(y_test, return_counts=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_3 (Conv2D)           (None, 249, 249, 16)      800       \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 249, 249, 16)     64        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_4 (Activation)   (None, 249, 249, 16)      0         \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPooling  (None, 124, 124, 16)     0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 124, 124, 16)      0         \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 124, 124, 32)      12832     \n",
      "                                                                 \n",
      " batch_normalization_5 (Batc  (None, 124, 124, 32)     128       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_5 (Activation)   (None, 124, 124, 32)      0         \n",
      "                                                                 \n",
      " max_pooling2d_4 (MaxPooling  (None, 62, 62, 32)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 62, 62, 32)        0         \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 62, 62, 64)        51264     \n",
      "                                                                 \n",
      " batch_normalization_6 (Batc  (None, 62, 62, 64)       256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_6 (Activation)   (None, 62, 62, 64)        0         \n",
      "                                                                 \n",
      " max_pooling2d_5 (MaxPooling  (None, 31, 31, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_6 (Dropout)         (None, 31, 31, 64)        0         \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 61504)             0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 64)                3936320   \n",
      "                                                                 \n",
      " batch_normalization_7 (Batc  (None, 64)               256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_7 (Activation)   (None, 64)                0         \n",
      "                                                                 \n",
      " dropout_7 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,001,985\n",
      "Trainable params: 4,001,633\n",
      "Non-trainable params: 352\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "cnn_model = None\n",
    "cnn_model = create_model(image_shape=(255, 255, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "earlystopping = EarlyStopping(monitor='val_accuracy', patience=10)\n",
    "filepath = \"A:\\TechieYan projects\\AI\\Fabric_Fault_detection\\Fabric 2\\model\\weights_best.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "callbacks_list = [checkpoint, earlystopping]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "288/288 [==============================] - ETA: 0s - loss: 0.5936 - accuracy: 0.6764\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.65938, saving model to A:\\TechieYan projects\\AI\\Fabric_Fault_detection\\Fabric 2\\model\\weights_best.hdf5\n",
      "288/288 [==============================] - 304s 1s/step - loss: 0.5936 - accuracy: 0.6764 - val_loss: 0.9034 - val_accuracy: 0.6594\n",
      "Epoch 2/10\n",
      "288/288 [==============================] - ETA: 0s - loss: 0.4465 - accuracy: 0.7955\n",
      "Epoch 00002: val_accuracy improved from 0.65938 to 0.70938, saving model to A:\\TechieYan projects\\AI\\Fabric_Fault_detection\\Fabric 2\\model\\weights_best.hdf5\n",
      "288/288 [==============================] - 260s 901ms/step - loss: 0.4465 - accuracy: 0.7955 - val_loss: 1.1755 - val_accuracy: 0.7094\n",
      "Epoch 3/10\n",
      "288/288 [==============================] - ETA: 0s - loss: 0.3718 - accuracy: 0.8375\n",
      "Epoch 00003: val_accuracy did not improve from 0.70938\n",
      "288/288 [==============================] - 257s 892ms/step - loss: 0.3718 - accuracy: 0.8375 - val_loss: 0.9447 - val_accuracy: 0.6906\n",
      "Epoch 4/10\n",
      "288/288 [==============================] - ETA: 0s - loss: 0.3089 - accuracy: 0.8701\n",
      "Epoch 00004: val_accuracy improved from 0.70938 to 0.86875, saving model to A:\\TechieYan projects\\AI\\Fabric_Fault_detection\\Fabric 2\\model\\weights_best.hdf5\n",
      "288/288 [==============================] - 255s 887ms/step - loss: 0.3089 - accuracy: 0.8701 - val_loss: 0.3386 - val_accuracy: 0.8687\n",
      "Epoch 5/10\n",
      "288/288 [==============================] - ETA: 0s - loss: 0.2222 - accuracy: 0.9160\n",
      "Epoch 00005: val_accuracy did not improve from 0.86875\n",
      "288/288 [==============================] - 274s 953ms/step - loss: 0.2222 - accuracy: 0.9160 - val_loss: 0.3688 - val_accuracy: 0.8438\n",
      "Epoch 6/10\n",
      "288/288 [==============================] - ETA: 0s - loss: 0.1842 - accuracy: 0.9326\n",
      "Epoch 00006: val_accuracy improved from 0.86875 to 0.94063, saving model to A:\\TechieYan projects\\AI\\Fabric_Fault_detection\\Fabric 2\\model\\weights_best.hdf5\n",
      "288/288 [==============================] - 296s 1s/step - loss: 0.1842 - accuracy: 0.9326 - val_loss: 0.1181 - val_accuracy: 0.9406\n",
      "Epoch 7/10\n",
      "288/288 [==============================] - ETA: 0s - loss: 0.1632 - accuracy: 0.9417\n",
      "Epoch 00007: val_accuracy did not improve from 0.94063\n",
      "288/288 [==============================] - 320s 1s/step - loss: 0.1632 - accuracy: 0.9417 - val_loss: 0.2706 - val_accuracy: 0.8938\n",
      "Epoch 8/10\n",
      "288/288 [==============================] - ETA: 0s - loss: 0.1378 - accuracy: 0.9500\n",
      "Epoch 00008: val_accuracy did not improve from 0.94063\n",
      "288/288 [==============================] - 284s 985ms/step - loss: 0.1378 - accuracy: 0.9500 - val_loss: 0.1426 - val_accuracy: 0.9312\n",
      "Epoch 9/10\n",
      "288/288 [==============================] - ETA: 0s - loss: 0.1551 - accuracy: 0.9399\n",
      "Epoch 00009: val_accuracy did not improve from 0.94063\n",
      "288/288 [==============================] - 282s 979ms/step - loss: 0.1551 - accuracy: 0.9399 - val_loss: 0.3626 - val_accuracy: 0.8531\n",
      "Epoch 10/10\n",
      "288/288 [==============================] - ETA: 0s - loss: 0.1185 - accuracy: 0.9556\n",
      "Epoch 00010: val_accuracy improved from 0.94063 to 0.95938, saving model to A:\\TechieYan projects\\AI\\Fabric_Fault_detection\\Fabric 2\\model\\weights_best.hdf5\n",
      "288/288 [==============================] - 283s 982ms/step - loss: 0.1185 - accuracy: 0.9556 - val_loss: 0.1156 - val_accuracy: 0.9594\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x28d48a63310>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# No need to run this, if you are loading the model, if not you cna run this to train your model again and save another weights file.\n",
    "# cnn_model.fit(X_train, y_train, batch_size=10, epochs=10, validation_split=0.1, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_model = tf.keras.models.load_model(\"path-of_saved_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.13624490797519684, 0.949999988079071)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score, acc = my_model.evaluate(X_test, y_test, verbose=0)\n",
    "score, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = cnn_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.9378993 ],\n",
       "       [0.02002826],\n",
       "       [0.9989196 ],\n",
       "       [0.67123127],\n",
       "       [0.4215867 ],\n",
       "       [0.907607  ],\n",
       "       [0.0552637 ],\n",
       "       [0.9459765 ],\n",
       "       [0.9992254 ],\n",
       "       [0.9969977 ],\n",
       "       [0.00795773],\n",
       "       [0.09753716],\n",
       "       [0.9976649 ],\n",
       "       [0.9638246 ],\n",
       "       [0.99827945],\n",
       "       [0.1274018 ],\n",
       "       [0.9992366 ],\n",
       "       [0.51289654],\n",
       "       [0.9973637 ],\n",
       "       [0.01850489],\n",
       "       [0.04367355],\n",
       "       [0.04167131],\n",
       "       [0.7128489 ],\n",
       "       [0.99977744],\n",
       "       [0.00329408],\n",
       "       [0.37138253],\n",
       "       [0.9211412 ],\n",
       "       [0.98227155],\n",
       "       [0.00913477],\n",
       "       [0.21792486],\n",
       "       [0.9991604 ],\n",
       "       [0.10970008],\n",
       "       [0.99946034],\n",
       "       [0.6862777 ],\n",
       "       [0.01467851],\n",
       "       [0.7078783 ],\n",
       "       [0.99528897],\n",
       "       [0.7949919 ],\n",
       "       [0.01371315],\n",
       "       [0.84297264],\n",
       "       [0.63125163],\n",
       "       [0.8683301 ],\n",
       "       [0.9976413 ],\n",
       "       [0.05072001],\n",
       "       [0.9990978 ],\n",
       "       [0.9994143 ],\n",
       "       [0.92707044],\n",
       "       [0.03736183],\n",
       "       [0.99655056],\n",
       "       [0.9999945 ],\n",
       "       [0.9980027 ],\n",
       "       [0.9274353 ],\n",
       "       [0.9999373 ],\n",
       "       [0.03731173],\n",
       "       [0.8989424 ],\n",
       "       [0.02385643],\n",
       "       [0.8090856 ],\n",
       "       [0.9976281 ],\n",
       "       [0.01022872],\n",
       "       [0.02136046],\n",
       "       [0.07163295],\n",
       "       [0.9700972 ],\n",
       "       [0.02461094],\n",
       "       [0.032455  ],\n",
       "       [0.01368207],\n",
       "       [0.9998684 ],\n",
       "       [0.25177184],\n",
       "       [0.99991965],\n",
       "       [0.05690306],\n",
       "       [0.00377789],\n",
       "       [0.02765003],\n",
       "       [0.10480398],\n",
       "       [0.05437335],\n",
       "       [0.84593666],\n",
       "       [0.05127978],\n",
       "       [0.9904367 ],\n",
       "       [0.16573489],\n",
       "       [0.24526098],\n",
       "       [0.09404242],\n",
       "       [0.02444094],\n",
       "       [0.9539846 ],\n",
       "       [0.01059592],\n",
       "       [0.9721332 ],\n",
       "       [0.9726371 ],\n",
       "       [0.988495  ],\n",
       "       [0.05437222],\n",
       "       [0.08117688],\n",
       "       [0.94358075],\n",
       "       [0.19787878],\n",
       "       [0.9804536 ],\n",
       "       [0.99669087],\n",
       "       [0.9955268 ],\n",
       "       [0.99869037],\n",
       "       [0.8621245 ],\n",
       "       [0.04807088],\n",
       "       [0.42550266],\n",
       "       [0.9514029 ],\n",
       "       [0.93031895],\n",
       "       [0.01156601],\n",
       "       [0.9069128 ],\n",
       "       [0.98010814],\n",
       "       [0.9307319 ],\n",
       "       [0.05695057],\n",
       "       [0.87539613],\n",
       "       [0.38801113],\n",
       "       [0.82836163],\n",
       "       [0.03737402],\n",
       "       [0.97113377],\n",
       "       [0.99531245],\n",
       "       [0.17917892],\n",
       "       [0.9377005 ],\n",
       "       [0.08244303],\n",
       "       [0.9895427 ],\n",
       "       [0.9863709 ],\n",
       "       [0.99286014],\n",
       "       [0.9991872 ],\n",
       "       [0.03324819],\n",
       "       [0.30776834],\n",
       "       [0.98753256],\n",
       "       [0.03162456],\n",
       "       [1.        ],\n",
       "       [0.03247043],\n",
       "       [0.9627918 ],\n",
       "       [0.96210325],\n",
       "       [0.9322096 ],\n",
       "       [0.02813512],\n",
       "       [0.9360156 ],\n",
       "       [0.01099724],\n",
       "       [0.98686314],\n",
       "       [0.99478483],\n",
       "       [0.9986056 ],\n",
       "       [0.99716294],\n",
       "       [0.95761096],\n",
       "       [0.75244963],\n",
       "       [0.1497631 ],\n",
       "       [0.9625635 ],\n",
       "       [0.08928064],\n",
       "       [0.03862533],\n",
       "       [0.26056957],\n",
       "       [0.14235196],\n",
       "       [0.06438425],\n",
       "       [0.95985866],\n",
       "       [0.9800383 ],\n",
       "       [0.999999  ],\n",
       "       [0.6168645 ],\n",
       "       [0.26542267],\n",
       "       [0.87162435],\n",
       "       [0.06231043],\n",
       "       [0.03632367],\n",
       "       [0.994122  ],\n",
       "       [0.08483529],\n",
       "       [0.9340782 ],\n",
       "       [0.21706933],\n",
       "       [0.21588266],\n",
       "       [0.99169457],\n",
       "       [0.09984657],\n",
       "       [0.9934032 ],\n",
       "       [0.06653365],\n",
       "       [0.8890078 ],\n",
       "       [0.9985393 ],\n",
       "       [0.37817   ],\n",
       "       [0.32129726],\n",
       "       [0.06992489],\n",
       "       [0.14856276],\n",
       "       [0.03809336],\n",
       "       [0.048217  ],\n",
       "       [0.8208041 ],\n",
       "       [0.05587664],\n",
       "       [0.5785168 ],\n",
       "       [0.9995341 ],\n",
       "       [0.01570559],\n",
       "       [0.02298751],\n",
       "       [0.9997451 ],\n",
       "       [0.3010686 ],\n",
       "       [0.8538214 ],\n",
       "       [0.98297215],\n",
       "       [0.04132611],\n",
       "       [0.9449282 ],\n",
       "       [0.12790692],\n",
       "       [0.02923188],\n",
       "       [0.0811919 ],\n",
       "       [0.9030752 ],\n",
       "       [0.0269042 ],\n",
       "       [0.98750055],\n",
       "       [0.9669916 ],\n",
       "       [0.11321092],\n",
       "       [0.99373555],\n",
       "       [0.62050694],\n",
       "       [0.05068147],\n",
       "       [0.90750074],\n",
       "       [0.06431088],\n",
       "       [0.9489941 ],\n",
       "       [0.9982556 ],\n",
       "       [0.97854793],\n",
       "       [0.23644686],\n",
       "       [0.9956565 ],\n",
       "       [0.06525922],\n",
       "       [0.9541634 ],\n",
       "       [0.97256607],\n",
       "       [0.02022088],\n",
       "       [0.9490747 ],\n",
       "       [0.9484046 ],\n",
       "       [0.04949895],\n",
       "       [0.9937109 ],\n",
       "       [0.01461762],\n",
       "       [0.9802848 ],\n",
       "       [0.9989222 ],\n",
       "       [0.64259994],\n",
       "       [0.00547725],\n",
       "       [0.29872513],\n",
       "       [0.67215914],\n",
       "       [0.9998438 ],\n",
       "       [0.98653555],\n",
       "       [0.9916419 ],\n",
       "       [0.97986937],\n",
       "       [0.4754521 ],\n",
       "       [0.08112893],\n",
       "       [0.09622076],\n",
       "       [0.05263314],\n",
       "       [0.8777042 ],\n",
       "       [0.2515369 ],\n",
       "       [0.98154414],\n",
       "       [0.0307644 ],\n",
       "       [0.17674688],\n",
       "       [0.16425958],\n",
       "       [0.9298315 ],\n",
       "       [0.14408925],\n",
       "       [0.05718765],\n",
       "       [0.97916514],\n",
       "       [0.93790066],\n",
       "       [0.99999493],\n",
       "       [0.9909857 ],\n",
       "       [0.00206488],\n",
       "       [0.03384787],\n",
       "       [0.14651027],\n",
       "       [0.00614569],\n",
       "       [0.9918559 ],\n",
       "       [0.93456066],\n",
       "       [0.9832367 ],\n",
       "       [0.99759203],\n",
       "       [0.9999984 ],\n",
       "       [0.07313094],\n",
       "       [0.99522626],\n",
       "       [0.1448547 ],\n",
       "       [0.05507973],\n",
       "       [0.96972597],\n",
       "       [0.99996436],\n",
       "       [0.7036452 ],\n",
       "       [0.16567612],\n",
       "       [0.9904977 ],\n",
       "       [0.02455628],\n",
       "       [0.38607123],\n",
       "       [0.7729999 ],\n",
       "       [0.9985384 ],\n",
       "       [0.14153212],\n",
       "       [0.07620111],\n",
       "       [0.10377839],\n",
       "       [0.04828027],\n",
       "       [0.06333873],\n",
       "       [0.97528005],\n",
       "       [0.10812774],\n",
       "       [0.9983702 ],\n",
       "       [0.9805098 ],\n",
       "       [0.93681234],\n",
       "       [0.09587991],\n",
       "       [0.01181367],\n",
       "       [0.90089023],\n",
       "       [0.9968941 ],\n",
       "       [0.03245783],\n",
       "       [0.61338973],\n",
       "       [0.9979745 ],\n",
       "       [0.06087953],\n",
       "       [0.9564056 ],\n",
       "       [0.96687806],\n",
       "       [0.99999094],\n",
       "       [0.9990431 ],\n",
       "       [0.83957314],\n",
       "       [0.09372693],\n",
       "       [0.998569  ],\n",
       "       [0.00500211],\n",
       "       [0.05653355],\n",
       "       [0.99790955],\n",
       "       [0.99851596],\n",
       "       [0.08560267],\n",
       "       [0.16428012],\n",
       "       [0.9972384 ],\n",
       "       [0.99927604],\n",
       "       [0.02606234],\n",
       "       [0.55610704],\n",
       "       [0.99810946],\n",
       "       [0.98355466],\n",
       "       [0.23606214],\n",
       "       [0.01134434],\n",
       "       [0.96155804],\n",
       "       [0.9987218 ],\n",
       "       [0.80458415],\n",
       "       [0.06393924],\n",
       "       [0.44299856],\n",
       "       [0.9429592 ],\n",
       "       [0.9992474 ],\n",
       "       [0.20894685],\n",
       "       [0.991521  ],\n",
       "       [0.94844705],\n",
       "       [0.03000417],\n",
       "       [0.90580535],\n",
       "       [0.9988611 ],\n",
       "       [0.7565949 ],\n",
       "       [0.9935628 ],\n",
       "       [0.69856435],\n",
       "       [0.00559846],\n",
       "       [0.99968994],\n",
       "       [0.6274985 ],\n",
       "       [0.910564  ],\n",
       "       [0.13293585],\n",
       "       [0.9862262 ],\n",
       "       [0.02319092],\n",
       "       [0.9839567 ],\n",
       "       [0.89871407],\n",
       "       [0.91537714],\n",
       "       [0.81347656],\n",
       "       [0.938025  ],\n",
       "       [0.998825  ],\n",
       "       [0.06538919],\n",
       "       [0.9722527 ],\n",
       "       [0.01308534],\n",
       "       [0.20322236],\n",
       "       [0.01704192],\n",
       "       [1.        ],\n",
       "       [0.9993069 ],\n",
       "       [0.8759611 ],\n",
       "       [0.98390543],\n",
       "       [0.98373055],\n",
       "       [0.02513337],\n",
       "       [0.24105877],\n",
       "       [0.98311424],\n",
       "       [0.07849926],\n",
       "       [0.9978351 ],\n",
       "       [0.09835276],\n",
       "       [0.17776927],\n",
       "       [0.10978094],\n",
       "       [0.98637867],\n",
       "       [0.9903009 ],\n",
       "       [0.16636437],\n",
       "       [0.08442014],\n",
       "       [0.96659076],\n",
       "       [0.1679962 ],\n",
       "       [0.09784085],\n",
       "       [0.03312397],\n",
       "       [0.23077038],\n",
       "       [0.02215311],\n",
       "       [0.9838066 ],\n",
       "       [0.00378978],\n",
       "       [0.16610202],\n",
       "       [0.9628273 ],\n",
       "       [0.02850342],\n",
       "       [0.99961925],\n",
       "       [0.7162828 ],\n",
       "       [0.94644463],\n",
       "       [0.9740926 ],\n",
       "       [0.5557879 ],\n",
       "       [0.87288666],\n",
       "       [0.01889491],\n",
       "       [0.04480177],\n",
       "       [0.3287982 ],\n",
       "       [0.98541903],\n",
       "       [0.9620919 ],\n",
       "       [0.06878147],\n",
       "       [0.17075974],\n",
       "       [0.995442  ],\n",
       "       [0.0319339 ],\n",
       "       [0.03214815],\n",
       "       [0.3363657 ],\n",
       "       [0.00378826],\n",
       "       [0.9861082 ],\n",
       "       [0.04601452],\n",
       "       [0.9893997 ],\n",
       "       [0.67737275],\n",
       "       [0.01155806],\n",
       "       [0.09556365],\n",
       "       [0.04937509],\n",
       "       [0.01292744],\n",
       "       [0.14222795],\n",
       "       [0.9888388 ],\n",
       "       [0.06321603],\n",
       "       [0.7210952 ],\n",
       "       [0.99480605],\n",
       "       [0.9931724 ],\n",
       "       [0.666499  ],\n",
       "       [0.9995021 ],\n",
       "       [0.98901033],\n",
       "       [0.02366349],\n",
       "       [0.61118805],\n",
       "       [0.9999813 ],\n",
       "       [0.02489626],\n",
       "       [0.99436295],\n",
       "       [0.9918966 ],\n",
       "       [0.9898127 ],\n",
       "       [0.0841203 ],\n",
       "       [0.9951724 ],\n",
       "       [0.0303157 ],\n",
       "       [0.8293142 ],\n",
       "       [0.9365065 ],\n",
       "       [0.0388194 ],\n",
       "       [0.02358094],\n",
       "       [0.03384131],\n",
       "       [0.99292976],\n",
       "       [0.07682157],\n",
       "       [0.36207616],\n",
       "       [0.9622265 ],\n",
       "       [0.9884865 ],\n",
       "       [0.00366548],\n",
       "       [0.03909329],\n",
       "       [0.99823225],\n",
       "       [0.9856547 ],\n",
       "       [0.9384558 ],\n",
       "       [0.998814  ],\n",
       "       [0.20431277],\n",
       "       [0.13495332],\n",
       "       [0.99239075],\n",
       "       [0.96752465],\n",
       "       [0.9909574 ],\n",
       "       [0.9971049 ],\n",
       "       [0.04296446],\n",
       "       [0.03284577],\n",
       "       [0.94920456],\n",
       "       [0.99200004],\n",
       "       [0.5174045 ],\n",
       "       [0.16302377],\n",
       "       [0.9431623 ],\n",
       "       [0.42312032],\n",
       "       [0.02099016],\n",
       "       [0.9999666 ],\n",
       "       [0.9971782 ],\n",
       "       [0.0102914 ],\n",
       "       [0.01509815],\n",
       "       [0.11001199],\n",
       "       [0.0228948 ],\n",
       "       [0.99925023],\n",
       "       [0.0139907 ],\n",
       "       [0.9996462 ],\n",
       "       [0.10844445],\n",
       "       [0.05772054],\n",
       "       [0.06667504],\n",
       "       [0.04420879],\n",
       "       [0.13469607],\n",
       "       [0.9999624 ],\n",
       "       [0.99736154],\n",
       "       [0.04724211],\n",
       "       [0.901278  ],\n",
       "       [0.97528255],\n",
       "       [0.03947377],\n",
       "       [0.9996637 ],\n",
       "       [0.8909552 ],\n",
       "       [0.9686327 ],\n",
       "       [0.8948126 ],\n",
       "       [0.02258578],\n",
       "       [0.03341597],\n",
       "       [0.99999326],\n",
       "       [0.9874797 ],\n",
       "       [0.10955819],\n",
       "       [0.9801767 ],\n",
       "       [0.06387672],\n",
       "       [0.1856325 ],\n",
       "       [0.04166573],\n",
       "       [0.18004054],\n",
       "       [0.06347454],\n",
       "       [0.00632975],\n",
       "       [0.99492383],\n",
       "       [0.9963517 ],\n",
       "       [0.9705056 ],\n",
       "       [0.0240398 ],\n",
       "       [0.99966955],\n",
       "       [0.9978525 ],\n",
       "       [0.9525369 ],\n",
       "       [0.18623704],\n",
       "       [0.5488583 ],\n",
       "       [0.29645872],\n",
       "       [0.94501156],\n",
       "       [0.36432388],\n",
       "       [0.9487427 ],\n",
       "       [0.00875461],\n",
       "       [0.989084  ],\n",
       "       [0.938576  ],\n",
       "       [0.99553823],\n",
       "       [0.98151845],\n",
       "       [0.31692815],\n",
       "       [0.04952773],\n",
       "       [0.02155325],\n",
       "       [0.62845623],\n",
       "       [0.8882963 ],\n",
       "       [0.998282  ],\n",
       "       [0.02802411],\n",
       "       [0.99997735],\n",
       "       [0.16303277],\n",
       "       [0.9982765 ],\n",
       "       [0.04042116],\n",
       "       [0.00666961],\n",
       "       [0.8391277 ],\n",
       "       [0.31423375],\n",
       "       [0.06437528],\n",
       "       [0.9975299 ],\n",
       "       [0.02545291],\n",
       "       [0.21517521],\n",
       "       [0.01077327],\n",
       "       [0.4695445 ],\n",
       "       [0.03090701],\n",
       "       [0.98147213],\n",
       "       [0.03118044],\n",
       "       [0.9314221 ],\n",
       "       [0.03188324],\n",
       "       [0.97046137],\n",
       "       [0.26366353],\n",
       "       [0.06975317],\n",
       "       [0.04323474],\n",
       "       [0.01728365],\n",
       "       [0.04602906],\n",
       "       [0.02934182],\n",
       "       [0.8942212 ],\n",
       "       [0.02571714],\n",
       "       [0.05266252],\n",
       "       [0.9996079 ],\n",
       "       [0.05458805],\n",
       "       [0.9986001 ],\n",
       "       [0.9987003 ],\n",
       "       [0.0330295 ],\n",
       "       [0.98216546],\n",
       "       [0.85431033],\n",
       "       [0.00624916],\n",
       "       [0.7377064 ],\n",
       "       [0.09093684],\n",
       "       [0.99991024],\n",
       "       [0.02022445],\n",
       "       [0.8802622 ],\n",
       "       [0.98758596],\n",
       "       [0.27506173],\n",
       "       [0.02188203],\n",
       "       [0.9992187 ],\n",
       "       [0.20227706],\n",
       "       [0.12892717],\n",
       "       [0.05887744],\n",
       "       [0.86638594],\n",
       "       [0.03652731],\n",
       "       [0.9385234 ],\n",
       "       [0.51029813],\n",
       "       [0.9503426 ],\n",
       "       [0.9365127 ],\n",
       "       [0.24520484],\n",
       "       [0.00463533],\n",
       "       [0.9351715 ],\n",
       "       [0.32692415],\n",
       "       [0.03806251],\n",
       "       [0.05821371],\n",
       "       [0.01529372],\n",
       "       [0.14653096],\n",
       "       [0.9999752 ],\n",
       "       [0.02607676],\n",
       "       [0.98414123],\n",
       "       [0.6565248 ],\n",
       "       [0.9472974 ],\n",
       "       [0.99763906],\n",
       "       [0.0508433 ],\n",
       "       [0.03389728],\n",
       "       [0.96704453],\n",
       "       [0.26477772],\n",
       "       [0.9758931 ],\n",
       "       [0.96337414],\n",
       "       [0.9649217 ],\n",
       "       [0.42301583],\n",
       "       [0.25886795],\n",
       "       [0.01744649],\n",
       "       [0.0116615 ],\n",
       "       [0.03825912],\n",
       "       [0.0185495 ],\n",
       "       [0.03319916],\n",
       "       [0.22230414],\n",
       "       [0.00560501],\n",
       "       [0.03039622],\n",
       "       [0.8648951 ],\n",
       "       [0.06322259],\n",
       "       [0.08399132],\n",
       "       [0.99986106],\n",
       "       [0.80955076],\n",
       "       [0.01241395],\n",
       "       [0.08994728],\n",
       "       [0.13169637],\n",
       "       [0.98603415],\n",
       "       [0.28547126],\n",
       "       [0.9990289 ],\n",
       "       [0.03099239],\n",
       "       [0.02503547],\n",
       "       [0.04166234],\n",
       "       [0.07135138],\n",
       "       [0.9573293 ],\n",
       "       [0.09954554],\n",
       "       [0.06695929],\n",
       "       [0.48252678],\n",
       "       [0.02917835],\n",
       "       [0.45855868],\n",
       "       [0.13702232],\n",
       "       [0.03965378],\n",
       "       [0.8367108 ],\n",
       "       [0.03017989],\n",
       "       [0.67159015],\n",
       "       [0.5133457 ],\n",
       "       [0.00308076],\n",
       "       [0.99993575],\n",
       "       [0.98601526],\n",
       "       [0.97164476],\n",
       "       [0.03871697],\n",
       "       [0.03973368],\n",
       "       [0.24792573],\n",
       "       [0.99955904],\n",
       "       [0.99872005],\n",
       "       [0.9982799 ],\n",
       "       [0.9292692 ],\n",
       "       [0.94559056],\n",
       "       [0.8133816 ],\n",
       "       [0.99977976],\n",
       "       [0.00824681],\n",
       "       [0.9786616 ],\n",
       "       [0.9996983 ],\n",
       "       [0.08724073],\n",
       "       [0.8503785 ],\n",
       "       [0.06328547],\n",
       "       [0.99910164],\n",
       "       [0.99779415],\n",
       "       [0.9991789 ],\n",
       "       [0.5035679 ],\n",
       "       [0.04377189],\n",
       "       [0.21098435],\n",
       "       [0.06657296],\n",
       "       [0.0235149 ],\n",
       "       [0.9378954 ],\n",
       "       [0.859042  ],\n",
       "       [0.9716799 ],\n",
       "       [0.04321671],\n",
       "       [0.8694199 ],\n",
       "       [0.99996185],\n",
       "       [0.16892841],\n",
       "       [0.9999374 ],\n",
       "       [0.04260498],\n",
       "       [0.02935001],\n",
       "       [0.9989214 ],\n",
       "       [0.9934622 ],\n",
       "       [0.994943  ],\n",
       "       [0.03341943],\n",
       "       [0.99796116],\n",
       "       [0.99891436],\n",
       "       [0.06768048],\n",
       "       [0.24441981],\n",
       "       [0.98943365],\n",
       "       [0.5691673 ],\n",
       "       [0.06631497],\n",
       "       [0.17368299],\n",
       "       [0.03469309],\n",
       "       [0.9995035 ],\n",
       "       [0.13567281],\n",
       "       [0.9967028 ],\n",
       "       [0.99248856],\n",
       "       [0.0777781 ],\n",
       "       [0.06450018],\n",
       "       [0.01571354],\n",
       "       [0.99906975],\n",
       "       [0.5234121 ],\n",
       "       [0.9998534 ],\n",
       "       [0.20030975],\n",
       "       [0.21139061],\n",
       "       [0.03172153],\n",
       "       [0.9817123 ],\n",
       "       [0.22378638],\n",
       "       [0.02520987],\n",
       "       [0.26561096],\n",
       "       [0.5670756 ],\n",
       "       [0.994655  ],\n",
       "       [0.99707544],\n",
       "       [0.99068093],\n",
       "       [0.25193   ],\n",
       "       [0.0837867 ],\n",
       "       [0.06407017],\n",
       "       [0.01147586],\n",
       "       [0.5643965 ],\n",
       "       [0.99924386],\n",
       "       [0.9917474 ],\n",
       "       [0.9963292 ],\n",
       "       [0.00729179],\n",
       "       [0.04303667],\n",
       "       [0.99647593],\n",
       "       [0.0367218 ],\n",
       "       [0.96455723],\n",
       "       [0.1356974 ],\n",
       "       [0.0446685 ],\n",
       "       [0.9997579 ],\n",
       "       [0.09374702],\n",
       "       [0.9996834 ],\n",
       "       [0.99997514],\n",
       "       [0.05132163],\n",
       "       [0.13752109],\n",
       "       [0.9774294 ],\n",
       "       [0.9207438 ],\n",
       "       [0.99953175],\n",
       "       [0.98756385],\n",
       "       [0.06218177],\n",
       "       [0.99897003],\n",
       "       [0.01460844],\n",
       "       [0.99133134],\n",
       "       [0.60980064],\n",
       "       [0.87442386],\n",
       "       [0.03728518],\n",
       "       [0.02614301],\n",
       "       [0.8480818 ],\n",
       "       [0.8097805 ],\n",
       "       [0.05073804],\n",
       "       [0.6643892 ],\n",
       "       [0.9732514 ],\n",
       "       [0.11140686],\n",
       "       [0.02899748],\n",
       "       [0.04576266],\n",
       "       [0.6786045 ],\n",
       "       [0.07763579],\n",
       "       [0.06132555],\n",
       "       [0.1876573 ],\n",
       "       [0.27895343],\n",
       "       [0.6223756 ],\n",
       "       [0.17777434],\n",
       "       [0.02991727],\n",
       "       [0.6891392 ],\n",
       "       [0.00482649],\n",
       "       [0.04404545],\n",
       "       [0.9880352 ],\n",
       "       [0.9608742 ],\n",
       "       [0.9973204 ],\n",
       "       [0.94302136],\n",
       "       [0.86176074],\n",
       "       [0.0068005 ],\n",
       "       [0.9875841 ],\n",
       "       [0.13699669],\n",
       "       [0.02239132],\n",
       "       [0.99430573],\n",
       "       [0.89566875],\n",
       "       [0.9955105 ],\n",
       "       [0.9789403 ],\n",
       "       [0.1087622 ],\n",
       "       [0.9999992 ],\n",
       "       [0.99362254],\n",
       "       [0.03577167],\n",
       "       [0.8917652 ],\n",
       "       [0.9948733 ],\n",
       "       [0.0295155 ],\n",
       "       [0.47256327],\n",
       "       [0.16940373],\n",
       "       [0.12895331],\n",
       "       [0.10047513],\n",
       "       [0.9974769 ],\n",
       "       [0.9966847 ],\n",
       "       [0.06806007],\n",
       "       [0.21879187],\n",
       "       [0.02708471],\n",
       "       [0.9997954 ],\n",
       "       [0.998517  ],\n",
       "       [0.99915314],\n",
       "       [0.98870206],\n",
       "       [0.9990542 ],\n",
       "       [0.9996263 ],\n",
       "       [0.98874056],\n",
       "       [0.02457255],\n",
       "       [0.9999579 ],\n",
       "       [0.07287475],\n",
       "       [0.68011296],\n",
       "       [0.00988749],\n",
       "       [0.01868075],\n",
       "       [0.9787072 ],\n",
       "       [0.9866313 ],\n",
       "       [0.06800961],\n",
       "       [0.05906868],\n",
       "       [0.5162405 ],\n",
       "       [0.06729162],\n",
       "       [0.9961611 ],\n",
       "       [0.86221516],\n",
       "       [0.03410783],\n",
       "       [0.9772403 ],\n",
       "       [0.00666127],\n",
       "       [0.01488289],\n",
       "       [0.99652565],\n",
       "       [0.96640253],\n",
       "       [0.9977195 ],\n",
       "       [0.98863035],\n",
       "       [0.89707226],\n",
       "       [0.994357  ],\n",
       "       [0.99673784],\n",
       "       [0.9997382 ],\n",
       "       [0.9421394 ],\n",
       "       [0.10434666],\n",
       "       [0.0371626 ],\n",
       "       [0.9089262 ],\n",
       "       [0.02036428],\n",
       "       [0.03245375],\n",
       "       [0.97886956],\n",
       "       [0.99697953],\n",
       "       [0.032455  ],\n",
       "       [0.9741571 ]], dtype=float32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "800"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "ab = y_pred.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "800"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count = len(y_pred)\n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(count):\n",
    "    if y_pred[i] > 0.5:\n",
    "        y_pred[i] = 1\n",
    "    else:\n",
    "        y_pred[i] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.]], dtype=float32)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 864x720 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize = (12,10))\n",
    "cf_matrix = confusion_matrix(y_pred,y_test)\n",
    "sns.heatmap(cf_matrix, annot=True, fmt=\"d\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing on Single Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test[73]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xi = np.array(X_test[73]).reshape(-1, 255, 255, 1)\n",
    "print(xi.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = cnn_model.predict(xi)\n",
    "y_pred\n",
    "if y_pred > 0.5:\n",
    "    y_pred = 1\n",
    "    print(\"The Fabric is Defect.\")\n",
    "else:\n",
    "    y_pred = 0\n",
    "    print(\"The Fabric is good.\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "80bc14a6df0c4942103d0618f3b06907407c921bdce606ff519c2e4f7269e47f"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
